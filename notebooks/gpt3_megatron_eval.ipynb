{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc253cb9",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d19e4",
   "metadata": {},
   "source": [
    "This notebook shows how to load a GPT-Megatron model on a single node with 8 V100 GPUs.\n",
    "\n",
    "https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/megatron_gpt_eval.py\n",
    "\n",
    "https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6865e8a",
   "metadata": {},
   "source": [
    "## Setting the initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c13a57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-03-23 20:35:38 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-03-23 20:35:38 __init__:22] `pynini` is not installed ! \n",
      "    Please run the `nemo_text_processing/setup.sh` scriptprior to usage of this toolkit.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nemo.collections.nlp.data.language_modeling.megatron.request_dataset import GPTRequestDataset\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\n",
    "from nemo.collections.nlp.modules.common.megatron.megatron_init import fake_initialize_model_parallel\n",
    "from nemo.collections.nlp.parts.nlp_overrides import NLPDDPPlugin\n",
    "from nemo.utils import logging\n",
    "from nemo.utils.app_state import AppState\n",
    "from nemo.utils.model_utils import inject_model_parallel_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa38e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa9bb286",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"/5b_checkpoints/checkpoints\"\n",
    "checkpoint_name = \"megatron_gpt--val_loss=1.78-step=32121-consumed_samples=46254240.0-last.ckpt\"\n",
    "# I have a node with 8 V100 GPUs so not sure what should I set for these variables\n",
    "devices = 1\n",
    "num_nodes = 2\n",
    "# The below comes from the checkpoint\n",
    "tensor_model_parallel_size=2\n",
    "pipeline_model_parallel_size=1\n",
    "precision=16\n",
    "hparams_file=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec56c35",
   "metadata": {},
   "source": [
    "\"devices * num_nodes should equal tensor_model_parallel_size * pipeline_model_parallel_size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5b12ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5b_checkpoints:\r\n",
      "checkpoints\r\n",
      "\r\n",
      "/5b_checkpoints/checkpoints:\r\n",
      "mp_rank_00  mp_rank_01\r\n",
      "\r\n",
      "/5b_checkpoints/checkpoints/mp_rank_00:\r\n",
      "'megatron_gpt--val_loss=1.78-step=32121-consumed_samples=46254240.0-last.ckpt'\r\n",
      "\r\n",
      "/5b_checkpoints/checkpoints/mp_rank_01:\r\n",
      "'megatron_gpt--val_loss=1.78-step=32121-consumed_samples=46254240.0-last.ckpt'\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R /5b_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca625218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-03-23 20:36:33 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:324: LightningDeprecationWarning: Passing <nemo.collections.nlp.parts.nlp_overrides.NLPDDPPlugin object at 0x7f67ce8a6610> `strategy` to the `plugins` flag in Trainer has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy=<nemo.collections.nlp.parts.nlp_overrides.NLPDDPPlugin object at 0x7f67ce8a6610>)` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    " trainer = Trainer(\n",
    "        plugins=[NLPDDPPlugin()],\n",
    "        devices=devices,\n",
    "        num_nodes=num_nodes,\n",
    "        accelerator='gpu',\n",
    "        precision=precision,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "003f26eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-03-23 20:36:34 megatron_init:186] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2022-03-23 20:36:34 megatron_init:189] All data parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2022-03-23 20:36:34 megatron_init:190] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2022-03-23 20:36:34 megatron_init:198] Rank 0 has model parallel group: [0, 1]\n",
      "[NeMo I 2022-03-23 20:36:34 megatron_init:199] All model parallel group ranks: [[0, 1]]\n",
      "[NeMo I 2022-03-23 20:36:34 megatron_init:209] Rank 0 has tensor model parallel group: [0, 1]\n",
      "[NeMo I 2022-03-23 20:36:34 megatron_init:213] All tensor model parallel group ranks: [[0, 1]]\n",
      "[NeMo I 2022-03-23 20:36:34 megatron_init:214] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2022-03-23 20:36:34 megatron_init:228] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2022-03-23 20:36:34 megatron_init:240] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2022-03-23 20:36:34 megatron_init:246] All pipeline model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2022-03-23 20:36:34 megatron_init:247] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2022-03-23 20:36:34 megatron_init:248] All embedding group ranks: [[0], [1]]\n",
      "[NeMo I 2022-03-23 20:36:34 megatron_init:249] Rank 0 has embedding rank: 0\n"
     ]
    }
   ],
   "source": [
    "app_state = AppState()\n",
    "if tensor_model_parallel_size > 1 or pipeline_model_parallel_size > 1:\n",
    "    app_state.pipeline_model_parallel_size = pipeline_model_parallel_size\n",
    "    app_state.tensor_model_parallel_size = tensor_model_parallel_size\n",
    "    app_state.model_parallel_size = tensor_model_parallel_size * pipeline_model_parallel_size\n",
    "    (\n",
    "        app_state.tensor_model_parallel_rank,\n",
    "        app_state.pipeline_model_parallel_rank,\n",
    "        app_state.model_parallel_size,\n",
    "        _,\n",
    "    ) = fake_initialize_model_parallel(\n",
    "        world_size=app_state.model_parallel_size,\n",
    "        rank=trainer.global_rank,\n",
    "        tensor_model_parallel_size_=app_state.tensor_model_parallel_size,\n",
    "        pipeline_model_parallel_size_=app_state.pipeline_model_parallel_size,\n",
    "    )\n",
    "    # inject model parallel rank\n",
    "checkpoint_path = inject_model_parallel_rank(os.path.join(checkpoint_dir, checkpoint_name))\n",
    "\n",
    "#     model = MegatronGPTModel.load_from_checkpoint(checkpoint_path, hparams_file=args.hparams_file, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9f35b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-03-23 20:36:55 megatron_init:186] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2022-03-23 20:36:55 megatron_init:189] All data parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2022-03-23 20:36:55 megatron_init:190] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2022-03-23 20:36:55 megatron_init:198] Rank 0 has model parallel group: [0, 1]\n",
      "[NeMo I 2022-03-23 20:36:55 megatron_init:199] All model parallel group ranks: [[0, 1]]\n",
      "[NeMo I 2022-03-23 20:36:55 megatron_init:209] Rank 0 has tensor model parallel group: [0, 1]\n",
      "[NeMo I 2022-03-23 20:36:55 megatron_init:213] All tensor model parallel group ranks: [[0, 1]]\n",
      "[NeMo I 2022-03-23 20:36:55 megatron_init:214] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2022-03-23 20:36:55 megatron_init:228] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2022-03-23 20:36:55 megatron_init:240] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2022-03-23 20:36:55 megatron_init:246] All pipeline model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2022-03-23 20:36:55 megatron_init:247] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2022-03-23 20:36:55 megatron_init:248] All embedding group ranks: [[0], [1]]\n",
      "[NeMo I 2022-03-23 20:36:55 megatron_init:249] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2022-03-23 20:36:55 tokenizer_utils:193] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m and custom vocab file: None\n",
      "[NeMo I 2022-03-23 20:36:55 tokenizer_utils:125] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-03-23 20:36:58 megatron_gpt_model:1203] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.\n"
     ]
    }
   ],
   "source": [
    "model = MegatronGPTModel.load_from_checkpoint(checkpoint_path, hparams_file=hparams_file, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2428ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    tokens, tokens_to_generate = batch[0]['data'], batch[0]['tokens_to_generate']\n",
    "    compute_logprobs = batch[0]['compute_logprobs']\n",
    "    lens = [len(token) for token in tokens]\n",
    "\n",
    "    tokens_pad = pad_sequence(tokens, batch_first=False, padding_value=50256)\n",
    "    data = []\n",
    "\n",
    "    if 'prompt_tags' in batch[0]:\n",
    "        # Keep track of soft prompt tags\n",
    "        prompt_tags = batch[0]['prompt_tags']\n",
    "\n",
    "        for token, lenn, prompt_tag in zip(tokens_pad.T, lens, prompt_tags):\n",
    "            data.append((token, lenn, tokens_to_generate, compute_logprobs, prompt_tag))\n",
    "    else:\n",
    "        for token, lenn in zip(tokens_pad.T, lens):\n",
    "            data.append((token, lenn, tokens_to_generate, compute_logprobs))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b87cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = []\n",
    "prompt = \"Translate German to English: Ich bin müde\"\n",
    "request.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f2da318",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_generate = 100\n",
    "compute_logprobs = True\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d656baef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GPTRequestDataset(request, model.tokenizer, tokens_to_generate, compute_logprobs)\n",
    "request_dl = DataLoader(dataset=pad_collate(dataset), batch_size=int(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dee05f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 23 20:37:10 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:1A:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    33W / 250W |  16573MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  On   | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    22W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE...  On   | 00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    23W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-PCIE...  On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    23W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-PCIE...  On   | 00000000:88:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    24W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-PCIE...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    22W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-PCIE...  On   | 00000000:B1:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    23W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-PCIE...  On   | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    24W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90900aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n"
     ]
    }
   ],
   "source": [
    " response = trainer.predict(model, request_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf0326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
