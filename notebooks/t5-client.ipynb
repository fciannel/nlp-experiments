{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91c3dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d08f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import T5Tokenizer # transformers-4.10.0-py3\n",
    "from utils.recover_bpe import recover_bpe\n",
    "import tritonclient.grpc as grpcclient\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import np_to_triton_dtype\n",
    "from sacrebleu import corpus_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5274a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b384ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(pred, ref):\n",
    "    bleu = corpus_bleu(pred, [ref], force=True)\n",
    "    print(\"       bleu score: {:6.2f}\".format(bleu.score))\n",
    "    print(\"       bleu counts: {}\".format(bleu.counts))\n",
    "    print(\"       bleu totals: {}\".format(bleu.totals))\n",
    "    print(\"       bleu precisions: {}\".format(bleu.precisions))\n",
    "    print(\"       bleu sys_len: {}; ref_len: {}\".format(bleu.sys_len, bleu.ref_len))\n",
    "    return bleu\n",
    "\n",
    "def prepare_tensor(name, input):\n",
    "    client_util = httpclient\n",
    "    t = client_util.InferInput(\n",
    "        name, input.shape, np_to_triton_dtype(input.dtype))\n",
    "    t.set_data_from_numpy(input)\n",
    "    return t\n",
    "\n",
    "class TranslationResult(object):\n",
    "    def __init__(self, name, frame_work):\n",
    "        self.name = name\n",
    "        self.frame_work = frame_work # FT or HF\n",
    "        self.file_name = name + \".txt\"\n",
    "\n",
    "        self.token_list = []\n",
    "        self.batch_ids_list = []\n",
    "        self.batch_seq_len_list = []\n",
    "        self.batch_num = 0\n",
    "        self.execution_time = 0.0  # seconds\n",
    "        self.sentence_num = 0\n",
    "        self.bleu_score = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b160805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_dict_args = {\"batch_size\":1,\n",
    "\"maximum_output_length\":128,\n",
    "\"source\":\"../test.en\",\n",
    "\"target\":\"../test.de\",\n",
    "\"model\":\"t5-3b\",\n",
    "\"beam_width\":1,\n",
    "\"sampling_topk\":1,\n",
    "\"sampling_topp\":0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba286376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(args_dict):\n",
    "    torch.set_printoptions(precision=6)\n",
    "    batch_size = args_dict['batch_size']\n",
    "    source_file = args_dict[\"source\"]\n",
    "    tgt_file = args_dict[\"target\"]\n",
    "    topk = args_dict['sampling_topk']\n",
    "    topp = args_dict['sampling_topp']\n",
    "    maximum_output_length = args_dict['maximum_output_length']\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(args_dict['model'])\n",
    "    fast_tokenizer = PreTrainedTokenizerFast.from_pretrained(args_dict['model'])\n",
    "\n",
    "    with open(source_file, 'r') as f:\n",
    "        src_text = recover_bpe(f.readlines())\n",
    "        src_text = [\"translate English to German: \" + line.strip() for line in src_text]\n",
    "\n",
    "    with open(tgt_file, 'r') as f:\n",
    "        tgt_text = recover_bpe(f.readlines())\n",
    "\n",
    "    translation_result_list = []\n",
    "    translation_result_list.append(TranslationResult(\"ft_triton\", \"FT\"))\n",
    "    client_util = httpclient\n",
    "    for i in range(len(translation_result_list)):\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        url = \"nvdl-smc-02:8000\"\n",
    "        model_name = \"fastertransformer\"\n",
    "        request_parallelism = 10\n",
    "        verbose = False\n",
    "        with client_util.InferenceServerClient(url,\n",
    "                                               concurrency=request_parallelism,\n",
    "                                               verbose=verbose) as client:\n",
    "            prev = 0\n",
    "            start_time = datetime.now()\n",
    "            results = []\n",
    "            while prev < 2:\n",
    "#             while prev < len(src_text):\n",
    "                input_texts = src_text[prev:prev+batch_size]\n",
    "                prev += batch_size\n",
    "                input_token = tokenizer(input_texts, return_tensors='pt', padding=True)\n",
    "                \n",
    "                input_ids = input_token.input_ids.numpy().astype(np.uint32)\n",
    "                mem_seq_len = torch.sum(input_token.attention_mask, dim=1).numpy().astype(np.uint32)\n",
    "                mem_seq_len = mem_seq_len.reshape([mem_seq_len.shape[0], 1])\n",
    "\n",
    "                # TODO(bhsueh) should be set to optional inputs in the future\n",
    "                runtime_top_k = (topk * np.ones([input_ids.shape[0], 1])).astype(np.uint32)\n",
    "                runtime_top_p = topp * np.ones([input_ids.shape[0], 1]).astype(np.float32)\n",
    "                beam_search_diversity_rate = 0.0 * np.ones([input_ids.shape[0], 1]).astype(np.float32)\n",
    "                temperature = 1.0 * np.ones([input_ids.shape[0], 1]).astype(np.float32)\n",
    "                len_penalty = 1.0 * np.ones([input_ids.shape[0], 1]).astype(np.float32)\n",
    "                repetition_penalty = 1.0 * np.ones([input_ids.shape[0], 1]).astype(np.float32)\n",
    "                random_seed = 0 * np.ones([input_ids.shape[0], 1]).astype(np.int32)\n",
    "                is_return_log_probs = True * np.ones([input_ids.shape[0], 1]).astype(bool)\n",
    "                max_output_len = (maximum_output_length * np.ones([input_ids.shape[0], 1])).astype(np.uint32)\n",
    "                bad_words_ids = np.array([[[0], [-1]]] * input_ids.shape[0], dtype=np.int32)\n",
    "                stop_words_ids = np.array([[[0], [-1]]] * input_ids.shape[0], dtype=np.int32)\n",
    "                beam_width = (args_dict['beam_width'] * np.ones([input_ids.shape[0], 1])).astype(np.uint32)\n",
    "                start_ids = 0 * np.ones([input_ids.shape[0], 1]).astype(np.uint32)\n",
    "                end_ids = 1 * np.ones([input_ids.shape[0], 1]).astype(np.uint32)\n",
    "\n",
    "                inputs = [\n",
    "                    prepare_tensor(\"input_ids\", input_ids),\n",
    "                    prepare_tensor(\"sequence_length\", mem_seq_len),\n",
    "                    prepare_tensor(\"runtime_top_k\", runtime_top_k),\n",
    "                    prepare_tensor(\"runtime_top_p\", runtime_top_p),\n",
    "                    prepare_tensor(\"beam_search_diversity_rate\", beam_search_diversity_rate),\n",
    "                    prepare_tensor(\"temperature\", temperature),\n",
    "                    prepare_tensor(\"len_penalty\", len_penalty),\n",
    "                    prepare_tensor(\"repetition_penalty\", repetition_penalty),\n",
    "                    prepare_tensor(\"random_seed\", random_seed),\n",
    "                    prepare_tensor(\"is_return_log_probs\", is_return_log_probs),\n",
    "                    prepare_tensor(\"max_output_len\", max_output_len),\n",
    "                    prepare_tensor(\"beam_width\", beam_width),\n",
    "                    prepare_tensor(\"start_id\", start_ids),\n",
    "                    prepare_tensor(\"end_id\", end_ids),\n",
    "                    prepare_tensor(\"bad_words_list\", bad_words_ids),\n",
    "                    prepare_tensor(\"stop_words_list\", stop_words_ids),\n",
    "                ]\n",
    "                print(input_texts)\n",
    "                print(\"set request\")\n",
    "                result = client.infer(model_name, inputs)\n",
    "                print(\"get request\")\n",
    "                results.append(result)\n",
    "                \n",
    "            for result in results:\n",
    "                ft_decoding_outputs = result.as_numpy(\"output_ids\")\n",
    "                ft_decoding_seq_lens = result.as_numpy(\"sequence_length\")\n",
    "                cum_log_probs = result.as_numpy(\"cum_log_probs\")\n",
    "                output_log_probs = result.as_numpy(\"output_log_probs\")\n",
    "                \n",
    "                translation_result_list[i].batch_ids_list.append(ft_decoding_outputs)\n",
    "                translation_result_list[i].batch_seq_len_list.append(ft_decoding_seq_lens)\n",
    "                \n",
    "                translation_result_list[i].sentence_num += len(input_token)\n",
    "                translation_result_list[i].batch_num += 1\n",
    "\n",
    "        stop_time = datetime.now()\n",
    "        translation_result_list[i].execution_time = (stop_time - start_time).total_seconds()\n",
    "        \n",
    "        for batch_token, batch_seq_len in zip(translation_result_list[i].batch_ids_list, translation_result_list[i].batch_seq_len_list):\n",
    "            for j in range(len(batch_token)):\n",
    "                translation_result_list[i].token_list.append(fast_tokenizer.decode(batch_token[j][0][:batch_seq_len[j][0]], skip_special_tokens=True))\n",
    "\n",
    "        translation_result_list[i].bleu_score = bleu_score(translation_result_list[i].token_list, tgt_text[:len(translation_result_list[i].token_list)])\n",
    "        with open(translation_result_list[i].name + \".txt\", 'w') as f:\n",
    "            for line in translation_result_list[i].token_list:\n",
    "                f.write(line)\n",
    "    \n",
    "    for t in translation_result_list:\n",
    "        if t.name.find(\"warmup\") != -1: \n",
    "            continue\n",
    "        print(\"[INFO] {} translates {} batches taking {:.2f} sec to translate {} tokens, BLEU score: {:.2f}, {:.0f} tokens/sec.\".format(\n",
    "                t.name, t.batch_num, t.execution_time, t.bleu_score.sys_len, t.bleu_score.score, t.bleu_score.sys_len / t.execution_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "235a39ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_348/3636957666.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dict_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_348/1952811042.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(args_dict)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreTrainedTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1670\u001b[0m                 \u001b[0;31m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m                 \u001b[0mfast_tokenizer_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFULL_TOKENIZER_FILE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m                 resolved_config_file = get_file_from_repo(\n\u001b[0m\u001b[1;32m   1673\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m                     \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_file_from_repo\u001b[0;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         resolved_file = cached_path(\n\u001b[0m\u001b[1;32m    679\u001b[0m             \u001b[0mresolved_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m    283\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    543\u001b[0m                     )\n\u001b[1;32m    544\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    546\u001b[0m                         \u001b[0;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                         \u001b[0;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "translate(_dict_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62bc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
